import logging
import re
import base64
import asyncio
from struct import pack
from hydrogram.file_id import FileId
from motor.motor_asyncio import AsyncIOMotorClient
from pymongo import TEXT
from pymongo.errors import DuplicateKeyError
from info import DATA_DATABASE_URL, DATABASE_NAME, COLLECTION_NAME, MAX_BTN, USE_CAPTION_FILTER

logger = logging.getLogger(__name__)

client = AsyncIOMotorClient(DATA_DATABASE_URL)
db = client[DATABASE_NAME]

# --- üóÑÔ∏è DUAL DATABASE COLLECTIONS ---
# 1. Primary DB (Safe Files)
col_main = db[COLLECTION_NAME]
# 2. Backup DB (Risky Files)
col_backup = db[f"{COLLECTION_NAME}_backup"]
# 3. Config DB (Router Rules & Settings)
col_config = db["bot_configuration"]

# --- ‚ö° COMPILED REGEX (Stronger & Optimized) ---
RE_SPECIAL = re.compile(r"[\.\+\-_]")
RE_USERNAMES = re.compile(r"@\w+")
RE_BRACKETS = re.compile(r"[\[\(\{].*?[\]\}\)]")
RE_EXTENSIONS = re.compile(r"(\.|\b)(mkv|mp4|avi|m4v|webm|flv|mov|wmv|3gp|mpg|mpeg|hevc|h264)\b", re.IGNORECASE)
RE_SPACES = re.compile(r"\s+")

# --- üõ†Ô∏è INDEXING HELPER ---
async def create_text_index():
    # ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§ï‡§≤‡•á‡§ï‡•ç‡§∂‡§® ‡§Æ‡•á‡§Ç ‡§á‡§Ç‡§°‡•á‡§ï‡•ç‡§∏ ‡§¨‡§®‡§æ‡§è‡§Å
    try:
        await col_main.create_index([("file_name", TEXT), ("caption", TEXT)], name="main_search_index")
        await col_backup.create_index([("file_name", TEXT), ("caption", TEXT)], name="backup_search_index")
    except Exception as e:
        logger.warning(f"Index Error: {e}")

# --- üßπ CLEANING FUNCTION ---
def clean_text(text):
    if not text: return ""
    text = str(text)
    text = RE_USERNAMES.sub("", text)
    text = RE_BRACKETS.sub("", text)
    text = RE_EXTENSIONS.sub("", text)
    text = RE_SPECIAL.sub(" ", text)
    text = RE_SPACES.sub(" ", text).strip()
    text = text.title()
    text = text.replace(" L ", " l ")
    return text

# --- üö¶ ROUTING & CONFIG LOGIC (NEW) ---
async def get_target_db(channel_id):
    """
    ‡§ö‡•á‡§ï ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§ï‡§ø ‡§á‡§∏ ‡§ö‡•à‡§®‡§≤ ‡§ï‡•Ä ‡§´‡§æ‡§á‡§≤ ‡§ï‡§ø‡§∏ DB ‡§Æ‡•á‡§Ç ‡§ú‡§æ‡§®‡•Ä ‡§ö‡§æ‡§π‡§ø‡§è‡•§
    Default: 'primary'
    """
    try:
        rule = await col_config.find_one({'_id': 'channel_routes'})
        if rule and str(channel_id) in rule.get('routes', {}):
            return rule['routes'][str(channel_id)] # returns 'backup' or 'primary'
    except: pass
    return 'primary'

async def set_route(channel_id, target):
    """
    ‡§è‡§°‡§Æ‡§ø‡§® ‡§™‡•à‡§®‡§≤ ‡§∏‡•á ‡§∞‡•Ç‡§ü ‡§∏‡•á‡§ü ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è‡•§
    target: 'primary' or 'backup'
    """
    await col_config.update_one(
        {'_id': 'channel_routes'},
        {'$set': {f"routes.{channel_id}": target}},
        upsert=True
    )

async def get_bot_settings():
    """
    ‡§™‡•Ç‡§∞‡•Ä ‡§¨‡•â‡§ü ‡§∏‡•á‡§ü‡§ø‡§Ç‡§ó‡•ç‡§∏ (Search Mode, Shortlink Status) ‡§≤‡§æ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è‡•§
    """
    stg = await col_config.find_one({'_id': 'main_settings'})
    if not stg: 
        # Default Settings
        return {'search_mode': 'hybrid', 'shortlink': False, 'auth_channel': None}
    return stg

# --- üíæ SAVE FILE (SMART) ---
async def save_file(media, target_db="primary"):
    """
    target_db: 'primary' (Default) or 'backup'
    """
    file_id = unpack_new_file_id(media.file_id)
    file_name = clean_text(media.file_name)
    file_caption = clean_text(media.caption)
    
    document = {
        '_id': file_id,
        'file_name': file_name,
        'file_size': media.file_size,
        'caption': file_caption,
        'file_type': media.file_type,
        'mime_type': media.mime_type
    }
    
    # ‡§∏‡§π‡•Ä ‡§ï‡§≤‡•á‡§ï‡•ç‡§∂‡§® ‡§ö‡•Å‡§®‡•á‡§Ç
    collection = col_backup if target_db == 'backup' else col_main
    
    try:
        await collection.insert_one(document)
        logger.info(f"‚úÖ Saved to [{target_db.upper()}]: {file_name[:50]}...") 
        return 'suc'
    except DuplicateKeyError:
        return 'dup'
    except Exception as e:
        logger.error(f"Save Error: {e}")
        return 'err'

# --- üîÑ UPDATE FILE ---
async def update_file(media):
    # ‡§Ö‡§™‡§°‡•á‡§ü ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§ú‡§ó‡§π ‡§ü‡•ç‡§∞‡§æ‡§à ‡§ï‡§∞‡•á‡§ó‡§æ ‡§ï‡•ç‡§Ø‡•ã‡§Ç‡§ï‡§ø ‡§π‡§Æ‡•á‡§Ç ‡§®‡§π‡•Ä‡§Ç ‡§™‡§§‡§æ ‡§´‡§æ‡§á‡§≤ ‡§ï‡§π‡§æ‡§Å ‡§π‡•à
    file_id = unpack_new_file_id(media.file_id)
    file_name = clean_text(media.file_name)
    file_caption = clean_text(media.caption)
    
    update_data = {'$set': {'file_name': file_name, 'caption': file_caption, 'file_size': media.file_size}}
    
    res1 = await col_main.update_one({'_id': file_id}, update_data)
    res2 = await col_backup.update_one({'_id': file_id}, update_data)
    
    if res1.modified_count or res2.modified_count:
        logger.info(f"üìù Updated: {file_name[:50]}...")
        return 'suc'
    return 'err'

# --- üîç SMART SEARCH (HYBRID) ---
async def get_search_results(query, max_results=MAX_BTN, offset=0, lang=None, mode="hybrid"):
    """
    mode: 'primary', 'backup', or 'hybrid'
    """
    query = str(query).strip().lower()
    query = RE_SPECIAL.sub(" ", query)
    query = RE_SPACES.sub(" ", query).strip()

    if not query: return [], "", 0

    # 1. ‡§∏‡§∞‡•ç‡§ö ‡§ï‡•ç‡§µ‡•á‡§∞‡•Ä ‡§¨‡§®‡§æ‡§ì
    if lang: filter_dict = {'$text': {'$search': f'"{query}" "{lang}"'}}
    else: filter_dict = {'$text': {'$search': query}}
    
    regex_fallback = False
    
    # 2. ‡§ï‡§≤‡•á‡§ï‡•ç‡§∂‡§® ‡§§‡§Ø ‡§ï‡§∞‡•ã
    collections_to_search = []
    if mode == 'primary': collections_to_search = [col_main]
    elif mode == 'backup': collections_to_search = [col_backup]
    else: collections_to_search = [col_main, col_backup] # Hybrid

    # 3. ‡§∏‡§∞‡•ç‡§ö ‡§è‡§ó‡•ç‡§ú‡•Ä‡§ï‡•ç‡§Ø‡•Ç‡§ü ‡§ï‡§∞‡•ã
    final_files = []
    total_count = 0
    
    for col in collections_to_search:
        try:
            # Text Search
            cursor_count = await col.count_documents(filter_dict)
            if cursor_count > 0:
                cursor = col.find(filter_dict, {'score': {'$meta': 'textScore'}}).sort([('score', {'$meta': 'textScore'})])
                # ‡§π‡§Æ ‡§Ö‡§≠‡•Ä ‡§≤‡§ø‡§Æ‡§ø‡§ü ‡§®‡§π‡•Ä‡§Ç ‡§≤‡§ó‡§æ ‡§∞‡§π‡•á, ‡§ï‡•ç‡§Ø‡•ã‡§Ç‡§ï‡§ø ‡§π‡§æ‡§á‡§¨‡•ç‡§∞‡§ø‡§° ‡§Æ‡•á‡§Ç ‡§Æ‡§∞‡•ç‡§ú ‡§ï‡§∞‡§®‡§æ ‡§π‡•ã‡§ó‡§æ
                # ‡§™‡§∞‡§´‡•â‡§∞‡•ç‡§Æ‡•á‡§Ç‡§∏ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§π‡§Æ ‡§∂‡•Å‡§∞‡•Ç ‡§ï‡•á 50-50 ‡§∞‡§ø‡§ú‡§≤‡•ç‡§ü ‡§≤‡•á ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç
                found = [doc async for doc in cursor.limit(100)] 
                final_files.extend(found)
            else:
                regex_fallback = True
        except:
            regex_fallback = True
            
    # 4. Regex Fallback (‡§Ö‡§ó‡§∞ ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§∏‡§∞‡•ç‡§ö ‡§´‡•á‡§≤ ‡§π‡•ã)
    if not final_files and regex_fallback:
        words = query.split()
        if len(words) > 0:
            pattern = "".join(f"(?=.*{re.escape(word)})" for word in words)
            filt = {'$or': [{'file_name': {'$regex': pattern, '$options': 'i'}}, {'caption': {'$regex': pattern, '$options': 'i'}}]} if USE_CAPTION_FILTER else {'file_name': {'$regex': pattern, '$options': 'i'}}
            
            for col in collections_to_search:
                try:
                    found = [doc async for doc in col.find(filt).sort('_id', -1).limit(50)]
                    final_files.extend(found)
                except: pass

    # 5. ‡§∞‡§ø‡§ú‡§≤‡•ç‡§ü‡•ç‡§∏ ‡§ï‡•ã ‡§Æ‡•à‡§®‡•á‡§ú ‡§ï‡§∞‡§®‡§æ (Pagination & Sorting)
    # ‡§π‡§æ‡§á‡§¨‡•ç‡§∞‡§ø‡§° ‡§Æ‡•ã‡§° ‡§Æ‡•á‡§Ç ‡§°‡•Å‡§™‡•ç‡§≤‡•Ä‡§ï‡•á‡§ü ‡§π‡•ã ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç (‡§µ‡•à‡§∏‡•á ID ‡§Ø‡•Ç‡§®‡§ø‡§ï ‡§π‡•à, ‡§™‡§∞ ‡§≤‡§ø‡§∏‡•ç‡§ü ‡§Æ‡•á‡§Ç ‡§Æ‡§ø‡§ï‡•ç‡§∏ ‡§π‡•ã ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç)
    # ‡§π‡§Æ ‡§´‡§æ‡§á‡§≤ ‡§®‡§æ‡§Æ ‡§ï‡•Ä ‡§≤‡§Ç‡§¨‡§æ‡§à ‡§Ø‡§æ ‡§Æ‡•à‡§ö ‡§∏‡•ç‡§ï‡•ã‡§∞ ‡§ï‡•á ‡§π‡§ø‡§∏‡§æ‡§¨ ‡§∏‡•á ‡§∏‡•â‡§∞‡•ç‡§ü ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç
    
    total_count = len(final_files)
    
    # Pagination Logic
    # ‡§ö‡•Ç‡§Ç‡§ï‡§ø ‡§π‡§Æ ‡§¶‡•ã DB ‡§∏‡•á ‡§≤‡§æ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç, 'skip/limit' DB ‡§≤‡•á‡§µ‡§≤ ‡§™‡§∞ ‡§ï‡§æ‡§Æ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§∞‡•á‡§ó‡§æ ‡§Ö‡§ó‡§∞ ‡§π‡§Æ ‡§Æ‡§∞‡•ç‡§ú ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç‡•§
    # ‡§á‡§∏‡§≤‡§ø‡§è ‡§π‡§Æ Python ‡§∏‡•ç‡§≤‡§æ‡§á‡§∏‡§ø‡§Ç‡§ó ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç‡§ó‡•á (Memory ‡§Æ‡•á‡§Ç)‡•§
    # ‡§®‡•ã‡§ü: ‡§¨‡§π‡•Å‡§§ ‡§¨‡§°‡§º‡•á ‡§°‡•á‡§ü‡§æ‡§¨‡•á‡§∏ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ø‡§π ‡§•‡•ã‡§°‡§º‡§æ ‡§≠‡§æ‡§∞‡•Ä ‡§π‡•ã ‡§∏‡§ï‡§§‡§æ ‡§π‡•à, ‡§≤‡•á‡§ï‡§ø‡§® 50-100 ‡§´‡§æ‡§á‡§≤‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§†‡•Ä‡§ï ‡§π‡•à‡•§
    
    start = offset
    end = offset + max_results
    sliced_files = final_files[start:end]
    
    next_offset = end if end < total_count else ""
    
    return sliced_files, next_offset, total_count

# --- üóëÔ∏è DELETE FILES ---
async def delete_files(query):
    if not query:
        r1 = await col_main.delete_many({})
        r2 = await col_backup.delete_many({})
        return r1.deleted_count + r2.deleted_count
    
    filt = {'file_name': {'$regex': query, '$options': 'i'}}
    r1 = await col_main.delete_many(filt)
    r2 = await col_backup.delete_many(filt)
    return r1.deleted_count + r2.deleted_count

async def get_file_details(query):
    # ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§ö‡•á‡§ï ‡§ï‡§∞‡•ã
    doc = await col_main.find_one({'_id': query})
    if not doc:
        doc = await col_backup.find_one({'_id': query})
    return doc

# --- FILE ID UTILS (Same as before) ---
def encode_file_id(s: bytes) -> str:
    r = b""
    n = 0
    for i in s + bytes([22]) + bytes([4]):
        if i == 0: n += 1
        else:
            if n: r += b"\x00" + bytes([n]); n = 0
            r += bytes([i])
    return base64.urlsafe_b64encode(r).decode().rstrip("=")

def unpack_new_file_id(new_file_id):
    decoded = FileId.decode(new_file_id)
    return encode_file_id(pack("<iiqq", int(decoded.file_type), decoded.dc_id, decoded.media_id, decoded.access_hash))

async def db_count_documents():
    c1 = await col_main.count_documents({})
    c2 = await col_backup.count_documents({})
    return c1 + c2 # Total files
